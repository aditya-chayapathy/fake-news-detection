{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>feature0</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature119</th>\n",
       "      <th>feature120</th>\n",
       "      <th>feature121</th>\n",
       "      <th>feature122</th>\n",
       "      <th>feature123</th>\n",
       "      <th>feature124</th>\n",
       "      <th>feature125</th>\n",
       "      <th>feature126</th>\n",
       "      <th>feature127</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>18079</td>\n",
       "      <td>1012</td>\n",
       "      <td>3782</td>\n",
       "      <td>13144</td>\n",
       "      <td>17021</td>\n",
       "      <td>2015</td>\n",
       "      <td>3901</td>\n",
       "      <td>2012</td>\n",
       "      <td>...</td>\n",
       "      <td>2007</td>\n",
       "      <td>1037</td>\n",
       "      <td>2603</td>\n",
       "      <td>3867</td>\n",
       "      <td>6226</td>\n",
       "      <td>5790</td>\n",
       "      <td>1998</td>\n",
       "      <td>6390</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>1037</td>\n",
       "      <td>9042</td>\n",
       "      <td>2155</td>\n",
       "      <td>3727</td>\n",
       "      <td>1996</td>\n",
       "      <td>2181</td>\n",
       "      <td>2206</td>\n",
       "      <td>1037</td>\n",
       "      <td>...</td>\n",
       "      <td>11510</td>\n",
       "      <td>2632</td>\n",
       "      <td>8865</td>\n",
       "      <td>5638</td>\n",
       "      <td>1013</td>\n",
       "      <td>21358</td>\n",
       "      <td>2361</td>\n",
       "      <td>1013</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>2095</td>\n",
       "      <td>2418</td>\n",
       "      <td>2515</td>\n",
       "      <td>2025</td>\n",
       "      <td>4025</td>\n",
       "      <td>2000</td>\n",
       "      <td>2022</td>\n",
       "      <td>2183</td>\n",
       "      <td>...</td>\n",
       "      <td>2009</td>\n",
       "      <td>1010</td>\n",
       "      <td>1000</td>\n",
       "      <td>2056</td>\n",
       "      <td>8398</td>\n",
       "      <td>1012</td>\n",
       "      <td>5371</td>\n",
       "      <td>3746</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>2006</td>\n",
       "      <td>1996</td>\n",
       "      <td>2305</td>\n",
       "      <td>1997</td>\n",
       "      <td>2244</td>\n",
       "      <td>2656</td>\n",
       "      <td>1010</td>\n",
       "      <td>2004</td>\n",
       "      <td>...</td>\n",
       "      <td>1012</td>\n",
       "      <td>2466</td>\n",
       "      <td>2506</td>\n",
       "      <td>2917</td>\n",
       "      <td>2058</td>\n",
       "      <td>1996</td>\n",
       "      <td>5179</td>\n",
       "      <td>2086</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>101</td>\n",
       "      <td>2343</td>\n",
       "      <td>8112</td>\n",
       "      <td>2003</td>\n",
       "      <td>2157</td>\n",
       "      <td>1517</td>\n",
       "      <td>2009</td>\n",
       "      <td>3544</td>\n",
       "      <td>1996</td>\n",
       "      <td>...</td>\n",
       "      <td>2597</td>\n",
       "      <td>2044</td>\n",
       "      <td>16039</td>\n",
       "      <td>2045</td>\n",
       "      <td>2001</td>\n",
       "      <td>2151</td>\n",
       "      <td>14398</td>\n",
       "      <td>1999</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  feature0  feature1  feature2  feature3  feature4  feature5  \\\n",
       "0           0       101     18079      1012      3782     13144     17021   \n",
       "1           1       101      1037      9042      2155      3727      1996   \n",
       "2           2       101      2095      2418      2515      2025      4025   \n",
       "3           3       101      2006      1996      2305      1997      2244   \n",
       "4           4       101      2343      8112      2003      2157      1517   \n",
       "\n",
       "   feature6  feature7  feature8  ...    feature119  feature120  feature121  \\\n",
       "0      2015      3901      2012  ...          2007        1037        2603   \n",
       "1      2181      2206      1037  ...         11510        2632        8865   \n",
       "2      2000      2022      2183  ...          2009        1010        1000   \n",
       "3      2656      1010      2004  ...          1012        2466        2506   \n",
       "4      2009      3544      1996  ...          2597        2044       16039   \n",
       "\n",
       "   feature122  feature123  feature124  feature125  feature126  feature127  \\\n",
       "0        3867        6226        5790        1998        6390         102   \n",
       "1        5638        1013       21358        2361        1013         102   \n",
       "2        2056        8398        1012        5371        3746         102   \n",
       "3        2917        2058        1996        5179        2086         102   \n",
       "4        2045        2001        2151       14398        1999         102   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      1  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # to ignore warnings being printed on the console\n",
    "\n",
    "base_path = \"../dataset/\"\n",
    "\n",
    "# train_df = pd.read_csv(base_path + \"feature_matrix_bf.csv\", sep=\",\")\n",
    "train_df = pd.read_csv(base_path + \"feature_matrix_pf.csv\", sep=\",\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature0</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature119</th>\n",
       "      <th>feature120</th>\n",
       "      <th>feature121</th>\n",
       "      <th>feature122</th>\n",
       "      <th>feature123</th>\n",
       "      <th>feature124</th>\n",
       "      <th>feature125</th>\n",
       "      <th>feature126</th>\n",
       "      <th>feature127</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>18079</td>\n",
       "      <td>1012</td>\n",
       "      <td>3782</td>\n",
       "      <td>13144</td>\n",
       "      <td>17021</td>\n",
       "      <td>2015</td>\n",
       "      <td>3901</td>\n",
       "      <td>2012</td>\n",
       "      <td>1037</td>\n",
       "      <td>...</td>\n",
       "      <td>2007</td>\n",
       "      <td>1037</td>\n",
       "      <td>2603</td>\n",
       "      <td>3867</td>\n",
       "      <td>6226</td>\n",
       "      <td>5790</td>\n",
       "      <td>1998</td>\n",
       "      <td>6390</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>1037</td>\n",
       "      <td>9042</td>\n",
       "      <td>2155</td>\n",
       "      <td>3727</td>\n",
       "      <td>1996</td>\n",
       "      <td>2181</td>\n",
       "      <td>2206</td>\n",
       "      <td>1037</td>\n",
       "      <td>2988</td>\n",
       "      <td>...</td>\n",
       "      <td>11510</td>\n",
       "      <td>2632</td>\n",
       "      <td>8865</td>\n",
       "      <td>5638</td>\n",
       "      <td>1013</td>\n",
       "      <td>21358</td>\n",
       "      <td>2361</td>\n",
       "      <td>1013</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>2095</td>\n",
       "      <td>2418</td>\n",
       "      <td>2515</td>\n",
       "      <td>2025</td>\n",
       "      <td>4025</td>\n",
       "      <td>2000</td>\n",
       "      <td>2022</td>\n",
       "      <td>2183</td>\n",
       "      <td>2092</td>\n",
       "      <td>...</td>\n",
       "      <td>2009</td>\n",
       "      <td>1010</td>\n",
       "      <td>1000</td>\n",
       "      <td>2056</td>\n",
       "      <td>8398</td>\n",
       "      <td>1012</td>\n",
       "      <td>5371</td>\n",
       "      <td>3746</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>2006</td>\n",
       "      <td>1996</td>\n",
       "      <td>2305</td>\n",
       "      <td>1997</td>\n",
       "      <td>2244</td>\n",
       "      <td>2656</td>\n",
       "      <td>1010</td>\n",
       "      <td>2004</td>\n",
       "      <td>1996</td>\n",
       "      <td>...</td>\n",
       "      <td>1012</td>\n",
       "      <td>2466</td>\n",
       "      <td>2506</td>\n",
       "      <td>2917</td>\n",
       "      <td>2058</td>\n",
       "      <td>1996</td>\n",
       "      <td>5179</td>\n",
       "      <td>2086</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>2343</td>\n",
       "      <td>8112</td>\n",
       "      <td>2003</td>\n",
       "      <td>2157</td>\n",
       "      <td>1517</td>\n",
       "      <td>2009</td>\n",
       "      <td>3544</td>\n",
       "      <td>1996</td>\n",
       "      <td>2111</td>\n",
       "      <td>...</td>\n",
       "      <td>2597</td>\n",
       "      <td>2044</td>\n",
       "      <td>16039</td>\n",
       "      <td>2045</td>\n",
       "      <td>2001</td>\n",
       "      <td>2151</td>\n",
       "      <td>14398</td>\n",
       "      <td>1999</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature0  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
       "0       101     18079      1012      3782     13144     17021      2015   \n",
       "1       101      1037      9042      2155      3727      1996      2181   \n",
       "2       101      2095      2418      2515      2025      4025      2000   \n",
       "3       101      2006      1996      2305      1997      2244      2656   \n",
       "4       101      2343      8112      2003      2157      1517      2009   \n",
       "\n",
       "   feature7  feature8  feature9  ...    feature119  feature120  feature121  \\\n",
       "0      3901      2012      1037  ...          2007        1037        2603   \n",
       "1      2206      1037      2988  ...         11510        2632        8865   \n",
       "2      2022      2183      2092  ...          2009        1010        1000   \n",
       "3      1010      2004      1996  ...          1012        2466        2506   \n",
       "4      3544      1996      2111  ...          2597        2044       16039   \n",
       "\n",
       "   feature122  feature123  feature124  feature125  feature126  feature127  \\\n",
       "0        3867        6226        5790        1998        6390         102   \n",
       "1        5638        1013       21358        2361        1013         102   \n",
       "2        2056        8398        1012        5371        3746         102   \n",
       "3        2917        2058        1996        5179        2086         102   \n",
       "4        2045        2001        2151       14398        1999         102   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      1  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.drop(train_df.columns[0], axis=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature0</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature119</th>\n",
       "      <th>feature120</th>\n",
       "      <th>feature121</th>\n",
       "      <th>feature122</th>\n",
       "      <th>feature123</th>\n",
       "      <th>feature124</th>\n",
       "      <th>feature125</th>\n",
       "      <th>feature126</th>\n",
       "      <th>feature127</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>101</td>\n",
       "      <td>4911</td>\n",
       "      <td>1024</td>\n",
       "      <td>8398</td>\n",
       "      <td>2074</td>\n",
       "      <td>2435</td>\n",
       "      <td>2296</td>\n",
       "      <td>2551</td>\n",
       "      <td>1010</td>\n",
       "      <td>3423</td>\n",
       "      <td>...</td>\n",
       "      <td>2731</td>\n",
       "      <td>2097</td>\n",
       "      <td>4374</td>\n",
       "      <td>2019</td>\n",
       "      <td>4469</td>\n",
       "      <td>1002</td>\n",
       "      <td>2423</td>\n",
       "      <td>1999</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>101</td>\n",
       "      <td>13258</td>\n",
       "      <td>2098</td>\n",
       "      <td>23564</td>\n",
       "      <td>20224</td>\n",
       "      <td>2050</td>\n",
       "      <td>1010</td>\n",
       "      <td>13229</td>\n",
       "      <td>3677</td>\n",
       "      <td>1997</td>\n",
       "      <td>...</td>\n",
       "      <td>9165</td>\n",
       "      <td>1012</td>\n",
       "      <td>2096</td>\n",
       "      <td>1996</td>\n",
       "      <td>4629</td>\n",
       "      <td>27461</td>\n",
       "      <td>1997</td>\n",
       "      <td>1996</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>101</td>\n",
       "      <td>3021</td>\n",
       "      <td>1051</td>\n",
       "      <td>1521</td>\n",
       "      <td>13875</td>\n",
       "      <td>2001</td>\n",
       "      <td>7283</td>\n",
       "      <td>2568</td>\n",
       "      <td>2075</td>\n",
       "      <td>2010</td>\n",
       "      <td>...</td>\n",
       "      <td>1998</td>\n",
       "      <td>9524</td>\n",
       "      <td>1012</td>\n",
       "      <td>1996</td>\n",
       "      <td>1019</td>\n",
       "      <td>27675</td>\n",
       "      <td>1010</td>\n",
       "      <td>2040</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>101</td>\n",
       "      <td>2660</td>\n",
       "      <td>4150</td>\n",
       "      <td>2034</td>\n",
       "      <td>2406</td>\n",
       "      <td>2000</td>\n",
       "      <td>4088</td>\n",
       "      <td>12702</td>\n",
       "      <td>5428</td>\n",
       "      <td>14853</td>\n",
       "      <td>...</td>\n",
       "      <td>21792</td>\n",
       "      <td>3593</td>\n",
       "      <td>12702</td>\n",
       "      <td>5428</td>\n",
       "      <td>14853</td>\n",
       "      <td>2007</td>\n",
       "      <td>3352</td>\n",
       "      <td>27061</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>101</td>\n",
       "      <td>2466</td>\n",
       "      <td>11637</td>\n",
       "      <td>1000</td>\n",
       "      <td>1045</td>\n",
       "      <td>2123</td>\n",
       "      <td>1005</td>\n",
       "      <td>1056</td>\n",
       "      <td>2113</td>\n",
       "      <td>2129</td>\n",
       "      <td>...</td>\n",
       "      <td>2142</td>\n",
       "      <td>2163</td>\n",
       "      <td>1997</td>\n",
       "      <td>2637</td>\n",
       "      <td>1010</td>\n",
       "      <td>1000</td>\n",
       "      <td>2002</td>\n",
       "      <td>2056</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature0  feature1  feature2  feature3  feature4  feature5  feature6  \\\n",
       "46        101      4911      1024      8398      2074      2435      2296   \n",
       "188       101     13258      2098     23564     20224      2050      1010   \n",
       "124       101      3021      1051      1521     13875      2001      7283   \n",
       "62        101      2660      4150      2034      2406      2000      4088   \n",
       "184       101      2466     11637      1000      1045      2123      1005   \n",
       "\n",
       "     feature7  feature8  feature9  ...    feature119  feature120  feature121  \\\n",
       "46       2551      1010      3423  ...          2731        2097        4374   \n",
       "188     13229      3677      1997  ...          9165        1012        2096   \n",
       "124      2568      2075      2010  ...          1998        9524        1012   \n",
       "62      12702      5428     14853  ...         21792        3593       12702   \n",
       "184      1056      2113      2129  ...          2142        2163        1997   \n",
       "\n",
       "     feature122  feature123  feature124  feature125  feature126  feature127  \\\n",
       "46         2019        4469        1002        2423        1999         102   \n",
       "188        1996        4629       27461        1997        1996         102   \n",
       "124        1996        1019       27675        1010        2040         102   \n",
       "62         5428       14853        2007        3352       27061         102   \n",
       "184        2637        1010        1000        2002        2056         102   \n",
       "\n",
       "     label  \n",
       "46       1  \n",
       "188      1  \n",
       "124      1  \n",
       "62       1  \n",
       "184      0  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.sample(frac=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 6.77150193e-02, 3.95043994e-04, 1.45380534e-03,\n",
       "        2.54462176e-04, 1.38484483e-03, 2.60136014e-04, 3.67511944e-05,\n",
       "        3.06006893e-01, 0.00000000e+00, 1.81418751e-02, 3.64327062e-02,\n",
       "        1.46598518e-03, 6.74412542e-02, 9.55309361e-02, 8.58985278e-02,\n",
       "        6.87627464e-02, 1.00408537e-01, 3.44615805e-02, 7.73999355e-02,\n",
       "        2.25770545e-01, 9.51961682e-02, 1.56271349e-01, 1.03361311e-01,\n",
       "        9.26966292e-02, 8.42296174e-02, 4.59765019e-02, 1.71045257e-01,\n",
       "        2.87473177e-01, 2.12453963e-01, 9.07302487e-02, 6.97902098e-02,\n",
       "        8.94146265e-02, 9.61297873e-02, 9.50249342e-02, 4.07719270e-02,\n",
       "        5.77375085e-02, 2.13303617e-01, 2.86142628e-01, 1.17873678e-01,\n",
       "        7.59070897e-02, 1.47094908e-01, 8.00045914e-02, 7.08527790e-02,\n",
       "        8.21701620e-02, 6.98251748e-02, 7.15053378e-02, 7.33536523e-02,\n",
       "        7.31336219e-02, 7.24674477e-02, 4.86312217e-01, 7.04905978e-02,\n",
       "        8.88175885e-02, 7.90673753e-02, 7.15232737e-02, 8.94743041e-02,\n",
       "        8.13607952e-02, 8.66948485e-02, 3.55708953e-02, 6.17879586e-02,\n",
       "        1.85485506e-01, 1.01615557e-01, 4.17488836e-01, 3.58598207e-02,\n",
       "        5.59083734e-02, 4.57334058e-01, 7.09285384e-02, 2.85658942e-01,\n",
       "        7.16566795e-02, 1.00000000e+00, 6.91282334e-02, 1.26193891e-01,\n",
       "        3.72130725e-02, 8.04871164e-02, 1.30279680e-01, 2.19139695e-01,\n",
       "        8.51630918e-02, 8.97369778e-02, 1.44417937e-01, 4.71830328e-02,\n",
       "        7.15797023e-02, 1.14752421e-01, 1.20336985e-01, 8.25970824e-02,\n",
       "        1.48616021e-01, 6.98137601e-02, 1.92165206e-01, 6.87891617e-02,\n",
       "        7.29336080e-02, 3.50865004e-02, 8.67875648e-02, 1.34923193e-01,\n",
       "        6.99817774e-02, 1.33480068e-01, 7.53136504e-02, 6.77643864e-02,\n",
       "        1.48857069e-01, 3.79390754e-02, 3.96313536e-01, 8.87250766e-02,\n",
       "        1.00171024e-01, 1.01503497e-01, 6.92784494e-02, 6.80389959e-02,\n",
       "        7.37511085e-02, 7.75770457e-02, 8.94779859e-02, 7.04374164e-02,\n",
       "        1.47915701e-01, 3.48793038e-02, 7.00413658e-02, 7.02165623e-02,\n",
       "        7.19574512e-02, 9.61977186e-01, 3.55708953e-02, 2.83792917e-01,\n",
       "        6.94993862e-02, 7.19841607e-02, 1.23703392e-01, 6.87627464e-02,\n",
       "        7.04726351e-02, 6.83623717e-02, 6.84381965e-02, 8.21054999e-02,\n",
       "        3.49616527e-02, 5.32867133e-02, 1.01215772e-01, 1.00000000e+00],\n",
       "       [0.00000000e+00, 3.79760377e-02, 1.65200215e-03, 6.94192048e-02,\n",
       "        1.87574975e-02, 1.98849514e-03, 5.42197778e-02, 3.64204337e-02,\n",
       "        4.19216431e-02, 1.75634235e-01, 6.02721281e-02, 3.58900145e-02,\n",
       "        4.05325092e-02, 7.63043235e-02, 5.16383438e-04, 3.82447175e-02,\n",
       "        5.16995241e-02, 3.66274565e-02, 6.92984851e-02, 7.35657720e-02,\n",
       "        1.26319426e-01, 7.04021976e-02, 9.44254680e-01, 8.03877527e-02,\n",
       "        7.37782591e-02, 8.53262474e-02, 9.01795611e-02, 2.09506796e-01,\n",
       "        7.05667147e-02, 2.36256991e-01, 9.06909034e-02, 3.53846154e-02,\n",
       "        7.88214062e-02, 8.07538642e-02, 7.27215612e-01, 2.90358970e-01,\n",
       "        3.82894837e-02, 1.11469227e-01, 6.80091315e-02, 1.10743131e-01,\n",
       "        7.59450433e-02, 8.74594689e-02, 8.13437404e-02, 7.12770471e-02,\n",
       "        7.87683243e-02, 7.01398601e-02, 9.86601705e-02, 3.45878566e-02,\n",
       "        6.86367325e-02, 8.29857459e-02, 7.52639517e-02, 8.71239976e-02,\n",
       "        7.28114632e-02, 2.13303617e-01, 3.00928083e-01, 7.84814102e-02,\n",
       "        7.12846262e-02, 5.66024217e-01, 3.55708953e-02, 8.23028583e-02,\n",
       "        8.16541658e-02, 2.30944447e-01, 3.49638246e-02, 1.91311435e-01,\n",
       "        7.56947249e-02, 1.65874040e-01, 7.72715966e-01, 3.56413327e-02,\n",
       "        7.39531379e-02, 6.93707994e-02, 5.23886612e-02, 3.69100514e-02,\n",
       "        7.46472127e-02, 8.07290617e-02, 1.00000000e+00, 4.08294802e-01,\n",
       "        8.45708000e-02, 1.72468626e-01, 9.55640839e-01, 9.40857703e-02,\n",
       "        7.15797023e-02, 1.17753376e-01, 3.56746637e-02, 8.25559893e-02,\n",
       "        8.78675520e-02, 6.98836437e-02, 7.62571984e-02, 6.79762913e-02,\n",
       "        3.03516571e-01, 5.28381852e-02, 4.55526770e-02, 7.25104737e-02,\n",
       "        2.79962153e-01, 3.25213989e-01, 3.80136729e-02, 6.79680869e-02,\n",
       "        7.75205212e-02, 7.49528075e-02, 8.43418358e-02, 1.05244460e-01,\n",
       "        6.98055914e-02, 7.06993007e-02, 5.21557481e-02, 1.00000000e+00,\n",
       "        5.63109666e-02, 7.10237336e-02, 7.09285384e-02, 3.09854195e-01,\n",
       "        3.60161181e-02, 7.00694133e-02, 7.01465330e-02, 2.52142882e-01,\n",
       "        5.28730837e-02, 3.68018977e-02, 8.32570261e-02, 6.50108137e-01,\n",
       "        7.20911199e-02, 7.18427379e-02, 3.54639753e-02, 6.79469748e-02,\n",
       "        7.02965415e-02, 4.95004944e-01, 7.06326076e-02, 2.49015859e-01,\n",
       "        5.24079320e-02, 7.03846154e-02, 7.42766343e-02, 1.00000000e+00],\n",
       "       [0.00000000e+00, 3.72272144e-02, 2.65720955e-01, 3.92890892e-02,\n",
       "        3.64971464e-02, 7.64860450e-02, 3.72737746e-02, 2.61190739e-01,\n",
       "        3.56263628e-02, 6.68804014e-01, 3.33250656e-01, 3.58538350e-02,\n",
       "        4.21569793e-01, 3.76866644e-02, 3.47338278e-01, 1.40974166e-01,\n",
       "        3.43303875e-02, 7.02965415e-02, 2.89852600e-01, 1.37707385e-01,\n",
       "        3.98034775e-02, 3.85398324e-01, 6.84861320e-02, 7.92055799e-02,\n",
       "        2.21503994e-01, 4.26842127e-02, 9.50565285e-02, 7.21241418e-02,\n",
       "        3.55296021e-02, 7.53648888e-02, 8.01857098e-02, 7.13986014e-02,\n",
       "        1.04245135e-01, 8.12381452e-02, 7.83759375e-02, 9.18577011e-02,\n",
       "        8.88240200e-02, 7.06326076e-02, 7.10075301e-02, 3.50444392e-02,\n",
       "        5.66266889e-01, 4.43431776e-02, 7.74028160e-02, 1.00799038e-01,\n",
       "        3.54211966e-02, 2.83636364e-01, 7.16844594e-02, 1.03763570e-01,\n",
       "        6.80281309e-02, 3.76119836e-02, 1.00000000e+00, 7.04555801e-02,\n",
       "        1.59003901e-01, 7.70786902e-02, 1.14630738e-01, 3.96529386e-02,\n",
       "        7.08064351e-02, 7.17202837e-02, 7.04726351e-02, 1.08128928e-01,\n",
       "        8.10865599e-02, 2.05635466e-01, 6.92352962e-02, 7.15424684e-02,\n",
       "        1.55721156e-01, 2.61159046e-01, 7.47663551e-02, 7.02965415e-02,\n",
       "        5.23628404e-01, 5.14370631e-02, 1.89542934e-01, 6.94923556e-01,\n",
       "        8.82097196e-01, 8.37937014e-02, 8.68436073e-01, 3.54490682e-02,\n",
       "        6.44751872e-02, 8.57830497e-02, 4.72807109e-01, 4.08156592e-01,\n",
       "        1.06186122e-01, 6.95675897e-02, 1.05047844e-01, 8.35011301e-02,\n",
       "        1.66336121e-01, 7.10017820e-02, 1.54715182e-01, 3.51227773e-02,\n",
       "        2.99071163e-01, 1.00000000e+00, 4.36096718e-02, 7.75235435e-02,\n",
       "        7.12082983e-02, 1.00073368e-01, 3.74427166e-01, 6.78322865e-02,\n",
       "        8.67596717e-02, 7.52119036e-02, 7.66337740e-02, 8.88138905e-02,\n",
       "        6.96659802e-02, 7.51398601e-02, 2.87935345e-01, 3.43761677e-02,\n",
       "        7.55246822e-02, 3.67339710e-02, 8.74169361e-02, 1.29076565e-01,\n",
       "        7.37082338e-02, 6.90679283e-02, 1.01626586e-01, 3.53356890e-02,\n",
       "        2.81989780e-01, 6.98015139e-02, 9.56540114e-02, 7.01878886e-02,\n",
       "        3.53635248e-02, 9.08287371e-02, 1.82891786e-01, 6.78789939e-02,\n",
       "        7.56850039e-02, 7.54202325e-02, 8.42448140e-02, 7.77190417e-02,\n",
       "        6.91632695e-02, 7.24125874e-02, 7.47200769e-02, 1.00000000e+00],\n",
       "       [0.00000000e+00, 3.76907716e-02, 3.68109176e-02, 2.20905721e-01,\n",
       "        5.57999200e-02, 1.00000000e+00, 3.69764763e-02, 7.19588387e-02,\n",
       "        3.69276219e-02, 3.93882672e-02, 4.69846219e-02, 3.58900145e-02,\n",
       "        6.41467570e-02, 3.75377053e-02, 4.70378368e-02, 7.94500548e-02,\n",
       "        8.73215500e-02, 7.04374164e-02, 1.04783677e-01, 7.16307736e-02,\n",
       "        9.23502092e-02, 7.70937522e-02, 3.45675639e-02, 7.94420144e-02,\n",
       "        8.34912684e-02, 8.51153570e-02, 4.59765019e-02, 1.00462379e-01,\n",
       "        7.05667147e-02, 6.86809439e-02, 8.76219704e-02, 6.98601399e-02,\n",
       "        7.90960452e-02, 2.22244643e-01, 8.00643971e-02, 8.66604891e-02,\n",
       "        8.65114868e-02, 6.88496486e-02, 6.86905857e-02, 9.93207394e-02,\n",
       "        7.76529528e-02, 4.43431776e-02, 1.43250689e-01, 7.84507142e-01,\n",
       "        7.01760539e-02, 2.25349650e-01, 7.47295264e-02, 7.58193213e-02,\n",
       "        6.75547741e-02, 7.41358674e-02, 7.80165913e-02, 3.63133382e-02,\n",
       "        1.17840085e-01, 6.84724841e-02, 8.67165944e-02, 7.84814102e-02,\n",
       "        1.00000000e+00, 9.77645626e-02, 7.24448827e-02, 8.18163389e-02,\n",
       "        8.10865599e-02, 9.95486565e-02, 7.80974141e-02, 3.23128167e-01,\n",
       "        7.39693844e-02, 3.43773354e-02, 1.33044313e-01, 7.81467916e-01,\n",
       "        7.18002081e-02, 3.50231349e-02, 9.92663521e-02, 5.45394115e-01,\n",
       "        2.47890645e-01, 1.15085286e-01, 8.08361872e-02, 6.99873897e-02,\n",
       "        8.52900114e-02, 1.00868145e-01, 5.25879058e-02, 4.93319630e-02,\n",
       "        7.43769051e-02, 7.33528850e-02, 7.85605325e-02, 4.15861927e-02,\n",
       "        9.56669828e-01, 6.99185856e-02, 7.79077871e-02, 6.78746825e-02,\n",
       "        7.21384943e-02, 5.29076634e-02, 1.24438687e-01, 1.14691875e-01,\n",
       "        5.34062237e-02, 4.94986549e-02, 7.50882729e-02, 1.35189272e-01,\n",
       "        7.42076784e-02, 7.40644779e-02, 3.02364550e-01, 8.87250766e-02,\n",
       "        6.97706886e-02, 8.12937063e-02, 6.90387316e-02, 6.79710588e-02,\n",
       "        7.37511085e-02, 7.32908254e-02, 1.04154081e-01, 7.07191660e-02,\n",
       "        7.19609172e-02, 8.32959215e-02, 7.15838183e-02, 3.62803065e-02,\n",
       "        7.87708138e-02, 6.96619807e-02, 7.02965415e-02, 2.27358746e-01,\n",
       "        9.73264220e-02, 7.08881346e-02, 1.20479394e-01, 7.09041468e-02,\n",
       "        5.33915616e-02, 6.91806744e-02, 6.86782102e-02, 7.98185431e-02,\n",
       "        6.96469288e-02, 1.28916084e-01, 7.37592846e-02, 1.00000000e+00],\n",
       "       [0.00000000e+00, 6.77150193e-02, 2.51391632e-04, 4.44464636e-01,\n",
       "        7.27034789e-05, 6.22114907e-01, 2.30480508e-01, 3.66776920e-02,\n",
       "        6.58366744e-02, 2.07654626e-01, 2.61958756e-01, 3.63965268e-02,\n",
       "        7.92424423e-04, 6.27490411e-02, 1.31724721e-01, 7.68544430e-02,\n",
       "        6.78450034e-02, 7.16348524e-02, 2.04073973e-01, 7.18816068e-02,\n",
       "        2.65919472e-01, 3.55708953e-02, 8.29689848e-02, 7.88115222e-02,\n",
       "        3.50954379e-02, 8.63385212e-02, 9.39924629e-02, 2.99635701e-01,\n",
       "        7.76374573e-02, 3.17726095e-01, 3.98174378e-02, 3.62587413e-02,\n",
       "        8.53735091e-02, 4.08006780e-02, 8.34805827e-02, 1.07731357e-01,\n",
       "        2.38039275e-01, 1.05091720e-01, 2.45562029e-01, 6.88722923e-02,\n",
       "        1.79292546e-01, 4.39488213e-02, 9.52708907e-02, 7.06052892e-02,\n",
       "        1.22641509e-01, 2.51958042e-01, 7.50877696e-02, 2.87592891e-01,\n",
       "        6.89072221e-02, 1.54546444e-01, 3.78205128e-02, 3.53678608e-02,\n",
       "        7.28843840e-02, 6.84381965e-02, 7.33507722e-02, 2.66499156e-01,\n",
       "        3.46005397e-02, 1.00773805e-01, 8.43488061e-02, 1.41293331e-01,\n",
       "        3.46807217e-01, 4.26878137e-02, 6.68016755e-01, 8.08972042e-02,\n",
       "        3.58760692e-01, 1.91555133e-01, 1.34288049e-01, 7.05430725e-02,\n",
       "        7.21949119e-02, 3.45503057e-02, 3.45468949e-02, 6.99016898e-02,\n",
       "        7.35418739e-02, 1.07464011e-01, 8.14426370e-02, 3.52038672e-02,\n",
       "        4.46334137e-02, 4.49286574e-01, 4.92099713e-01, 9.35718957e-02,\n",
       "        7.21534875e-02, 3.46814896e-02, 3.47732631e-02, 4.15861927e-02,\n",
       "        8.61860826e-02, 3.62346693e-02, 7.51201262e-02, 3.42421677e-02,\n",
       "        7.68368933e-02, 9.28923782e-02, 2.71113990e-01, 3.61657178e-02,\n",
       "        2.52558172e-01, 2.57275618e-01, 3.15453384e-01, 6.82736378e-02,\n",
       "        3.75823610e-02, 9.93818707e-02, 3.76093837e-02, 1.98321417e-01,\n",
       "        3.50075041e-02, 6.99300699e-02, 1.57563097e-01, 3.40704508e-02,\n",
       "        3.73928466e-02, 7.07049238e-02, 3.03969297e-01, 7.04726351e-02,\n",
       "        1.47844382e-01, 7.05874227e-02, 3.63528010e-02, 2.07500962e-01,\n",
       "        3.12267529e-01, 6.99410472e-02, 2.53821230e-01, 3.41308462e-02,\n",
       "        3.93807120e-01, 7.11002687e-02, 7.05775161e-02, 2.78178110e-01,\n",
       "        1.04775657e-01, 6.80555082e-02, 1.04543117e-01, 1.66685412e-01,\n",
       "        7.06142472e-02, 7.04895105e-02, 7.39440523e-02, 1.00000000e+00]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_df\n",
    "feature_matrix_df = train_data.iloc[:, :-1]\n",
    "labels_df = train_data.iloc[:, -1]\n",
    "feature_matrix = feature_matrix_df.values\n",
    "labels = labels_df.values\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(feature_matrix, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "transformed_train_data = MinMaxScaler().fit_transform(train_data)\n",
    "transformed_test_data = MinMaxScaler().fit_transform(test_data)\n",
    "# transformed_train_data = train_data\n",
    "# transformed_test_data = test_data\n",
    "\n",
    "transformed_train_data[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TRAINING DATA RESULTS:\n",
      "\n",
      "\n",
      "BaggingClassifier Classifier\n",
      "Best parameters set found:\n",
      "{'random_state': 10, 'oob_score': False, 'n_estimators': 14, 'max_samples': 0.7, 'max_features': 0.7}\n",
      "Best score: 0.565\n",
      "Scores on training set:\n",
      "0.565 (+/-0.198) for \"{'random_state': 10, 'oob_score': False, 'n_estimators': 14, 'max_samples': 0.7, 'max_features': 0.7}\"\n",
      "0.565 (+/-0.198) for \"{'random_state': 10, 'oob_score': True, 'n_estimators': 14, 'max_samples': 0.7, 'max_features': 0.7}\"\n",
      "0.553 (+/-0.166) for \"{'random_state': 10, 'oob_score': False, 'n_estimators': 10, 'max_samples': 0.8, 'max_features': 0.8}\"\n",
      "0.549 (+/-0.202) for \"{'random_state': 10, 'oob_score': False, 'n_estimators': 14, 'max_samples': 0.8, 'max_features': 0.7}\"\n",
      "0.524 (+/-0.201) for \"{'random_state': 10, 'oob_score': True, 'n_estimators': 38, 'max_samples': 0.7, 'max_features': 0.7}\"\n",
      "0.522 (+/-0.201) for \"{'random_state': 10, 'oob_score': True, 'n_estimators': 26, 'max_samples': 0.8, 'max_features': 0.7}\"\n",
      "0.509 (+/-0.261) for \"{'random_state': 10, 'oob_score': True, 'n_estimators': 18, 'max_samples': 0.8, 'max_features': 0.8}\"\n",
      "0.500 (+/-0.220) for \"{'random_state': 10, 'oob_score': True, 'n_estimators': 18, 'max_samples': 0.7, 'max_features': 0.8}\"\n",
      "0.496 (+/-0.247) for \"{'random_state': 10, 'oob_score': True, 'n_estimators': 38, 'max_samples': 0.7, 'max_features': 0.8}\"\n",
      "0.481 (+/-0.204) for \"{'random_state': 10, 'oob_score': True, 'n_estimators': 34, 'max_samples': 0.8, 'max_features': 0.8}\"\n",
      "\n",
      "\n",
      "MultinomialNB Classifier\n",
      "Best parameters set found:\n",
      "{'fit_prior': False, 'alpha': 2}\n",
      "Best score: 0.524\n",
      "Scores on training set:\n",
      "0.524 (+/-0.178) for \"{'fit_prior': False, 'alpha': 2}\"\n",
      "0.518 (+/-0.165) for \"{'fit_prior': False, 'alpha': 0.0001}\"\n",
      "0.518 (+/-0.165) for \"{'fit_prior': False, 'alpha': 0.001}\"\n",
      "0.518 (+/-0.165) for \"{'fit_prior': False, 'alpha': 0.1}\"\n",
      "0.491 (+/-0.186) for \"{'fit_prior': True, 'alpha': 2}\"\n",
      "0.484 (+/-0.194) for \"{'fit_prior': True, 'alpha': 0.001}\"\n",
      "0.484 (+/-0.194) for \"{'fit_prior': True, 'alpha': 1}\"\n",
      "0.484 (+/-0.194) for \"{'fit_prior': True, 'alpha': 0.1}\"\n",
      "0.484 (+/-0.194) for \"{'fit_prior': True, 'alpha': 0.01}\"\n",
      "0.484 (+/-0.194) for \"{'fit_prior': True, 'alpha': 0.0001}\"\n",
      "\n",
      "\n",
      "LogisticRegression Classifier\n",
      "Best parameters set found:\n",
      "{'warm_start': False, 'solver': 'newton-cg', 'random_state': 10, 'fit_intercept': True, 'C': 0.1}\n",
      "Best score: 0.522\n",
      "Scores on training set:\n",
      "0.522 (+/-0.216) for \"{'warm_start': False, 'solver': 'newton-cg', 'random_state': 10, 'fit_intercept': True, 'C': 0.1}\"\n",
      "0.498 (+/-0.214) for \"{'warm_start': True, 'solver': 'lbfgs', 'random_state': 10, 'fit_intercept': False, 'C': 1}\"\n",
      "0.480 (+/-0.213) for \"{'warm_start': True, 'solver': 'saga', 'random_state': 10, 'fit_intercept': False, 'C': 0.1}\"\n",
      "0.373 (+/-0.147) for \"{'warm_start': True, 'solver': 'sag', 'random_state': 10, 'fit_intercept': True, 'C': 0.01}\"\n",
      "0.373 (+/-0.147) for \"{'warm_start': False, 'solver': 'lbfgs', 'random_state': 10, 'fit_intercept': True, 'C': 0.01}\"\n",
      "0.338 (+/-0.011) for \"{'warm_start': True, 'solver': 'sag', 'random_state': 10, 'fit_intercept': True, 'C': 0.001}\"\n",
      "0.338 (+/-0.011) for \"{'warm_start': True, 'solver': 'liblinear', 'random_state': 10, 'fit_intercept': False, 'C': 0.001}\"\n",
      "0.338 (+/-0.011) for \"{'warm_start': False, 'solver': 'lbfgs', 'random_state': 10, 'fit_intercept': True, 'C': 0.001}\"\n",
      "0.338 (+/-0.011) for \"{'warm_start': False, 'solver': 'newton-cg', 'random_state': 10, 'fit_intercept': False, 'C': 0.001}\"\n",
      "0.338 (+/-0.011) for \"{'warm_start': False, 'solver': 'liblinear', 'random_state': 10, 'fit_intercept': False, 'C': 0.001}\"\n",
      "\n",
      "\n",
      "AdaBoostClassifier Classifier\n",
      "Best parameters set found:\n",
      "{'random_state': 10, 'n_estimators': 38, 'learning_rate': 1, 'algorithm': 'SAMME.R'}\n",
      "Best score: 0.513\n",
      "Scores on training set:\n",
      "0.513 (+/-0.236) for \"{'random_state': 10, 'n_estimators': 38, 'learning_rate': 1, 'algorithm': 'SAMME.R'}\"\n",
      "0.508 (+/-0.191) for \"{'random_state': 10, 'n_estimators': 30, 'learning_rate': 1, 'algorithm': 'SAMME.R'}\"\n",
      "0.460 (+/-0.176) for \"{'random_state': 10, 'n_estimators': 10, 'learning_rate': 0.1, 'algorithm': 'SAMME'}\"\n",
      "0.460 (+/-0.260) for \"{'random_state': 10, 'n_estimators': 34, 'learning_rate': 0.1, 'algorithm': 'SAMME.R'}\"\n",
      "0.440 (+/-0.276) for \"{'random_state': 10, 'n_estimators': 30, 'learning_rate': 1, 'algorithm': 'SAMME'}\"\n",
      "0.425 (+/-0.170) for \"{'random_state': 10, 'n_estimators': 30, 'learning_rate': 0.001, 'algorithm': 'SAMME.R'}\"\n",
      "0.423 (+/-0.277) for \"{'random_state': 10, 'n_estimators': 22, 'learning_rate': 1, 'algorithm': 'SAMME'}\"\n",
      "0.422 (+/-0.167) for \"{'random_state': 10, 'n_estimators': 14, 'learning_rate': 1, 'algorithm': 'SAMME.R'}\"\n",
      "0.420 (+/-0.168) for \"{'random_state': 10, 'n_estimators': 38, 'learning_rate': 0.001, 'algorithm': 'SAMME.R'}\"\n",
      "0.417 (+/-0.153) for \"{'random_state': 10, 'n_estimators': 30, 'learning_rate': 0.001, 'algorithm': 'SAMME'}\"\n",
      "\n",
      "\n",
      "RandomForestClassifier Classifier\n",
      "Best parameters set found:\n",
      "{'warm_start': True, 'random_state': 10, 'n_estimators': 10, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_depth': 10}\n",
      "Best score: 0.570\n",
      "Scores on training set:\n",
      "0.570 (+/-0.217) for \"{'warm_start': True, 'random_state': 10, 'n_estimators': 10, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_depth': 10}\"\n",
      "0.559 (+/-0.247) for \"{'warm_start': True, 'random_state': 10, 'n_estimators': 26, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': 10}\"\n",
      "0.559 (+/-0.247) for \"{'warm_start': False, 'random_state': 10, 'n_estimators': 26, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_depth': 10}\"\n",
      "0.549 (+/-0.182) for \"{'warm_start': False, 'random_state': 10, 'n_estimators': 14, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': 20}\"\n",
      "0.522 (+/-0.236) for \"{'warm_start': True, 'random_state': 10, 'n_estimators': 10, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_depth': 40}\"\n",
      "0.517 (+/-0.268) for \"{'warm_start': False, 'random_state': 10, 'n_estimators': 34, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': 30}\"\n",
      "0.517 (+/-0.268) for \"{'warm_start': False, 'random_state': 10, 'n_estimators': 34, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': 10}\"\n",
      "0.510 (+/-0.180) for \"{'warm_start': False, 'random_state': 10, 'n_estimators': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 20}\"\n",
      "0.507 (+/-0.244) for \"{'warm_start': True, 'random_state': 10, 'n_estimators': 26, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_depth': 30}\"\n",
      "0.496 (+/-0.227) for \"{'warm_start': False, 'random_state': 10, 'n_estimators': 26, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 40}\"\n",
      "\n",
      "\n",
      "GradientBoostingClassifier Classifier\n",
      "Best parameters set found:\n",
      "{'subsample': 0.8, 'random_state': 10, 'n_estimators': 26, 'min_samples_split': 3, 'min_samples_leaf': 40, 'max_features': 5, 'max_depth': 6}\n",
      "Best score: 0.544\n",
      "Scores on training set:\n",
      "0.544 (+/-0.162) for \"{'subsample': 0.8, 'random_state': 10, 'n_estimators': 26, 'min_samples_split': 3, 'min_samples_leaf': 40, 'max_features': 5, 'max_depth': 6}\"\n",
      "0.521 (+/-0.140) for \"{'subsample': 0.7, 'random_state': 10, 'n_estimators': 26, 'min_samples_split': 4, 'min_samples_leaf': 40, 'max_features': 5, 'max_depth': 6}\"\n",
      "0.512 (+/-0.260) for \"{'subsample': 0.75, 'random_state': 10, 'n_estimators': 10, 'min_samples_split': 4, 'min_samples_leaf': 56, 'max_features': 8, 'max_depth': 7}\"\n",
      "0.491 (+/-0.148) for \"{'subsample': 0.6, 'random_state': 10, 'n_estimators': 22, 'min_samples_split': 4, 'min_samples_leaf': 40, 'max_features': 5, 'max_depth': 3}\"\n",
      "0.477 (+/-0.216) for \"{'subsample': 0.6, 'random_state': 10, 'n_estimators': 30, 'min_samples_split': 2, 'min_samples_leaf': 44, 'max_features': 8, 'max_depth': 3}\"\n",
      "0.476 (+/-0.183) for \"{'subsample': 0.85, 'random_state': 10, 'n_estimators': 34, 'min_samples_split': 4, 'min_samples_leaf': 44, 'max_features': 8, 'max_depth': 5}\"\n",
      "0.474 (+/-0.145) for \"{'subsample': 0.8, 'random_state': 10, 'n_estimators': 38, 'min_samples_split': 2, 'min_samples_leaf': 48, 'max_features': 8, 'max_depth': 4}\"\n",
      "0.473 (+/-0.236) for \"{'subsample': 0.8, 'random_state': 10, 'n_estimators': 34, 'min_samples_split': 4, 'min_samples_leaf': 40, 'max_features': 9, 'max_depth': 7}\"\n",
      "0.450 (+/-0.240) for \"{'subsample': 0.75, 'random_state': 10, 'n_estimators': 14, 'min_samples_split': 4, 'min_samples_leaf': 40, 'max_features': 8, 'max_depth': 6}\"\n",
      "0.338 (+/-0.011) for \"{'subsample': 0.6, 'random_state': 10, 'n_estimators': 14, 'min_samples_split': 4, 'min_samples_leaf': 56, 'max_features': 6, 'max_depth': 7}\"\n",
      "\n",
      "\n",
      "DecisionTreeClassifier Classifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found:\n",
      "{'random_state': 10, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_depth': 16}\n",
      "Best score: 0.534\n",
      "Scores on training set:\n",
      "0.534 (+/-0.290) for \"{'random_state': 10, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_depth': 16}\"\n",
      "0.534 (+/-0.290) for \"{'random_state': 10, 'min_samples_split': 5, 'min_samples_leaf': 3, 'max_depth': 28}\"\n",
      "0.525 (+/-0.255) for \"{'random_state': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 16}\"\n",
      "0.519 (+/-0.243) for \"{'random_state': 10, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': 13}\"\n",
      "0.517 (+/-0.265) for \"{'random_state': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 25}\"\n",
      "0.516 (+/-0.258) for \"{'random_state': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 10}\"\n",
      "0.505 (+/-0.162) for \"{'random_state': 10, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_depth': 10}\"\n",
      "0.505 (+/-0.162) for \"{'random_state': 10, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_depth': 16}\"\n",
      "0.501 (+/-0.149) for \"{'random_state': 10, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': 19}\"\n",
      "0.501 (+/-0.149) for \"{'random_state': 10, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': 19}\"\n",
      "\n",
      "\n",
      "MLPClassifier Classifier\n",
      "Best parameters set found:\n",
      "{'solver': 'lbfgs', 'shuffle': True, 'random_state': 10, 'max_iter': 300, 'learning_rate_init': 0.1, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (30,), 'activation': 'identity'}\n",
      "Best score: 0.500\n",
      "Scores on training set:\n",
      "0.500 (+/-0.236) for \"{'solver': 'lbfgs', 'shuffle': True, 'random_state': 10, 'max_iter': 300, 'learning_rate_init': 0.1, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (30,), 'activation': 'identity'}\"\n",
      "0.499 (+/-0.186) for \"{'solver': 'lbfgs', 'shuffle': False, 'random_state': 10, 'max_iter': 300, 'learning_rate_init': 0.1, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (50, 50), 'activation': 'relu'}\"\n",
      "0.490 (+/-0.220) for \"{'solver': 'lbfgs', 'shuffle': True, 'random_state': 10, 'max_iter': 400, 'learning_rate_init': 0.001, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (70, 50, 30), 'activation': 'tanh'}\"\n",
      "0.489 (+/-0.229) for \"{'solver': 'lbfgs', 'shuffle': True, 'random_state': 10, 'max_iter': 400, 'learning_rate_init': 0.1, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (40, 40, 40), 'activation': 'tanh'}\"\n",
      "0.481 (+/-0.240) for \"{'solver': 'adam', 'shuffle': True, 'random_state': 10, 'max_iter': 400, 'learning_rate_init': 0.001, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (40, 40, 40), 'activation': 'identity'}\"\n",
      "0.466 (+/-0.194) for \"{'solver': 'lbfgs', 'shuffle': True, 'random_state': 10, 'max_iter': 300, 'learning_rate_init': 0.01, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (40, 40, 40), 'activation': 'identity'}\"\n",
      "0.434 (+/-0.242) for \"{'solver': 'adam', 'shuffle': True, 'random_state': 10, 'max_iter': 300, 'learning_rate_init': 0.1, 'learning_rate': 'constant', 'hidden_layer_sizes': (70, 30), 'activation': 'relu'}\"\n",
      "0.366 (+/-0.114) for \"{'solver': 'sgd', 'shuffle': False, 'random_state': 10, 'max_iter': 300, 'learning_rate_init': 0.001, 'learning_rate': 'constant', 'hidden_layer_sizes': (70, 50, 30), 'activation': 'relu'}\"\n",
      "0.338 (+/-0.011) for \"{'solver': 'adam', 'shuffle': True, 'random_state': 10, 'max_iter': 300, 'learning_rate_init': 0.001, 'learning_rate': 'invscaling', 'hidden_layer_sizes': (50, 50), 'activation': 'logistic'}\"\n",
      "0.338 (+/-0.011) for \"{'solver': 'adam', 'shuffle': False, 'random_state': 10, 'max_iter': 200, 'learning_rate_init': 0.01, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (70, 30), 'activation': 'logistic'}\"\n",
      "\n",
      "\n",
      "SVC Classifier\n",
      "Best parameters set found:\n",
      "{'random_state': 10, 'kernel': 'linear', 'degree': 9, 'C': 1}\n",
      "Best score: 0.505\n",
      "Scores on training set:\n",
      "0.505 (+/-0.207) for \"{'random_state': 10, 'kernel': 'linear', 'degree': 9, 'C': 1}\"\n",
      "0.505 (+/-0.207) for \"{'random_state': 10, 'kernel': 'linear', 'degree': 4, 'C': 1}\"\n",
      "0.502 (+/-0.188) for \"{'random_state': 10, 'kernel': 'linear', 'degree': 10, 'C': 6}\"\n",
      "0.496 (+/-0.186) for \"{'random_state': 10, 'kernel': 'linear', 'degree': 19, 'C': 8}\"\n",
      "0.496 (+/-0.186) for \"{'random_state': 10, 'kernel': 'linear', 'degree': 9, 'C': 8}\"\n",
      "0.414 (+/-0.188) for \"{'random_state': 10, 'kernel': 'rbf', 'degree': 2, 'C': 4}\"\n",
      "0.414 (+/-0.188) for \"{'random_state': 10, 'kernel': 'rbf', 'degree': 9, 'C': 4}\"\n",
      "0.392 (+/-0.187) for \"{'random_state': 10, 'kernel': 'sigmoid', 'degree': 12, 'C': 7}\"\n",
      "0.338 (+/-0.011) for \"{'random_state': 10, 'kernel': 'poly', 'degree': 5, 'C': 4}\"\n",
      "0.338 (+/-0.011) for \"{'random_state': 10, 'kernel': 'poly', 'degree': 10, 'C': 9}\"\n",
      "\n",
      "\n",
      "KNeighborsClassifier Classifier\n",
      "Best parameters set found:\n",
      "{'weights': 'distance', 'p': 1, 'n_neighbors': 21, 'metric': 'canberra'}\n",
      "Best score: 0.510\n",
      "Scores on training set:\n",
      "0.510 (+/-0.256) for \"{'weights': 'distance', 'p': 1, 'n_neighbors': 21, 'metric': 'canberra'}\"\n",
      "0.425 (+/-0.169) for \"{'weights': 'distance', 'p': 4, 'n_neighbors': 18, 'metric': 'sokalmichener'}\"\n",
      "0.420 (+/-0.137) for \"{'weights': 'distance', 'p': 3, 'n_neighbors': 25, 'metric': 'matching'}\"\n",
      "0.405 (+/-0.125) for \"{'weights': 'uniform', 'p': 3, 'n_neighbors': 3, 'metric': 'matching'}\"\n",
      "0.391 (+/-0.166) for \"{'weights': 'distance', 'p': 3, 'n_neighbors': 16, 'metric': 'dice'}\"\n",
      "0.384 (+/-0.168) for \"{'weights': 'distance', 'p': 4, 'n_neighbors': 18, 'metric': 'kulsinski'}\"\n",
      "0.363 (+/-0.079) for \"{'weights': 'uniform', 'p': 5, 'n_neighbors': 30, 'metric': 'rogerstanimoto'}\"\n",
      "0.361 (+/-0.120) for \"{'weights': 'uniform', 'p': 2, 'n_neighbors': 9, 'metric': 'rogerstanimoto'}\"\n",
      "0.356 (+/-0.075) for \"{'weights': 'uniform', 'p': 3, 'n_neighbors': 26, 'metric': 'matching'}\"\n",
      "0.353 (+/-0.128) for \"{'weights': 'uniform', 'p': 1, 'n_neighbors': 11, 'metric': 'kulsinski'}\"\n"
     ]
    }
   ],
   "source": [
    "def train_model(model_name, Model, tuned_parameters, X_train, y_train, fold_num=10, scoring_function=\"f1_macro\",\n",
    "                useRandomizedSearch=True):\n",
    "    if useRandomizedSearch == False:\n",
    "        clf = GridSearchCV(Model, tuned_parameters, cv=fold_num, scoring=scoring_function)\n",
    "    else:\n",
    "        clf = RandomizedSearchCV(Model, tuned_parameters, cv=fold_num, scoring=scoring_function)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found:\")\n",
    "    print(clf.best_params_)\n",
    "    print(\"Best score: %0.3f\" % clf.best_score_)\n",
    "    print(\"Scores on training set:\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    meanMap = {}\n",
    "    stdMap = {}\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        meanMap[str(params)] = mean\n",
    "        stdMap[str(params)] = std\n",
    "    for params in sorted(meanMap, key=meanMap.get, reverse=True):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (meanMap[params], stdMap[params] * 2, params))\n",
    "\n",
    "    return clf.best_params_\n",
    "\n",
    "models_dict = {}\n",
    "print(\"\\n\\nTRAINING DATA RESULTS:\")\n",
    "print(\"\\n\\nBaggingClassifier Classifier\")\n",
    "tuned_parameters = {'random_state': [10],\n",
    "                    'n_estimators': np.arange(10, 41, 4),\n",
    "                    'max_samples': [0.7, 0.8],\n",
    "                    'max_features': [0.7, 0.8],\n",
    "                    'oob_score': [True, False]}\n",
    "models_dict[\"BaggingClassifier()\"] = train_model(\"BaggingClassifier\", BaggingClassifier(),\n",
    "                                                tuned_parameters, transformed_train_data, train_labels,\n",
    "                                                useRandomizedSearch=True)\n",
    "\n",
    "print(\"\\n\\nMultinomialNB Classifier\")\n",
    "tuned_parameters = {'fit_prior': [True, False],\n",
    "                    'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 2]}\n",
    "models_dict[\"MultinomialNB()\"] = train_model(\"MultinomialNB\", MultinomialNB(), tuned_parameters,\n",
    "                                             transformed_train_data, train_labels, useRandomizedSearch=True)\n",
    "\n",
    "print(\"\\n\\nLogisticRegression Classifier\")\n",
    "tuned_parameters = {'random_state': [10],\n",
    "                    'C': [0.01, 0.1, 1, 0.001],\n",
    "                    'fit_intercept': [True, False],\n",
    "                    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                    'warm_start': [True, False]}\n",
    "models_dict[\"LogisticRegression()\"] = train_model(\"LogisticRegression\", LogisticRegression(), tuned_parameters,\n",
    "                                                  transformed_train_data, train_labels, useRandomizedSearch=True)\n",
    "\n",
    "print(\"\\n\\nAdaBoostClassifier Classifier\")\n",
    "tuned_parameters = {'random_state': [10],\n",
    "                    'n_estimators': range(10, 41, 4),\n",
    "                    'learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "                    'algorithm': ['SAMME.R', 'SAMME']}\n",
    "models_dict[\"AdaBoostClassifier()\"] = train_model(\"AdaBoostClassifier\", AdaBoostClassifier(), tuned_parameters,\n",
    "                                                  transformed_train_data, train_labels, useRandomizedSearch=True)\n",
    "\n",
    "print(\"\\n\\nRandomForestClassifier Classifier\")\n",
    "tuned_parameters = {'random_state': [10],\n",
    "                    'n_estimators': np.arange(10, 41, 4),\n",
    "                    'max_depth': [10, 20, 30, 40],\n",
    "                    'min_samples_split': [2, 3, 5, 10],\n",
    "                    'warm_start': [True, False],\n",
    "                    'min_samples_leaf': [1, 2, 4]}\n",
    "models_dict[\"RandomForestClassifier()\"] = train_model(\"RandomForestClassifier\", RandomForestClassifier(),\n",
    "                                                      tuned_parameters, transformed_train_data, train_labels,\n",
    "                                                      useRandomizedSearch=True)\n",
    "\n",
    "print(\"\\n\\nGradientBoostingClassifier Classifier\")\n",
    "tuned_parameters = {'random_state': [10],\n",
    "                    'n_estimators': range(10, 41, 4),\n",
    "                    'max_depth': range(3, 8),\n",
    "                    'min_samples_split': range(2, 5),\n",
    "                    'min_samples_leaf': range(40, 60, 4),\n",
    "                    'max_features': range(5, 10),\n",
    "                    'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9]}\n",
    "models_dict[\"GradientBoostingClassifier()\"] = train_model(\"GradientBoostingClassifier\",\n",
    "                                                          GradientBoostingClassifier(), tuned_parameters,\n",
    "                                                          transformed_train_data, train_labels,\n",
    "                                                          useRandomizedSearch=True)\n",
    "print(\"\\n\\nDecisionTreeClassifier Classifier\")\n",
    "tuned_parameters = {'random_state': [10],\n",
    "                    'max_depth': range(10, 30, 3),\n",
    "                    'min_samples_split': [2, 5, 10],\n",
    "                    'min_samples_leaf': [1, 2, 3, 4]}\n",
    "models_dict[\"DecisionTreeClassifier()\"] = train_model(\"DecisionTreeClassifier\", DecisionTreeClassifier(),\n",
    "                                                      tuned_parameters, transformed_train_data, train_labels,\n",
    "                                                      useRandomizedSearch=True)\n",
    "\n",
    "print(\"\\n\\nMLPClassifier Classifier\")\n",
    "tuned_parameters = {'random_state': [10],\n",
    "                    'max_iter': range(200, 500, 100),\n",
    "                    'hidden_layer_sizes': [(40, 40, 40), (50, 50, ), (70, 50, 30,), (70, 30,), (30,)],\n",
    "                    'activation': ['relu', 'identity', 'logistic', 'tanh'],\n",
    "                    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "                    'shuffle': [True, False],\n",
    "                    'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "                    'learning_rate': ['constant', 'invscaling', 'adaptive']}\n",
    "models_dict[\"MLPClassifier()\"] = train_model(\"MLPClassifier\", MLPClassifier(), tuned_parameters,\n",
    "                                             transformed_train_data, train_labels, useRandomizedSearch=True)\n",
    "\n",
    "print(\"\\n\\nSVC Classifier\")\n",
    "tuned_parameters = {'random_state': [10],\n",
    "                    'kernel': ['rbf', 'linear', 'poly', 'sigmoid'],\n",
    "                    'degree': np.arange(2, 20),\n",
    "                    'C': np.arange(1, 10)}\n",
    "models_dict[\"SVC()\"] = train_model(\"SVC\", SVC(), tuned_parameters, transformed_train_data, train_labels,\n",
    "                                   useRandomizedSearch=True)\n",
    "\n",
    "print(\"\\n\\nKNeighborsClassifier Classifier\")\n",
    "tuned_parameters = {'n_neighbors': np.arange(1, 31),\n",
    "                    'weights': [\"uniform\", \"distance\"],\n",
    "                    'p': np.arange(1, 6),\n",
    "                    'metric': [\"minkowski\", \"braycurtis\", \"canberra\", \"matching\", \"dice\", \"kulsinski\",\n",
    "                               \"rogerstanimoto\", \"russellrao\", \"sokalmichener\", \"sokalsneath\"]}\n",
    "models_dict[\"KNeighborsClassifier()\"] = train_model(\"KNeighborsClassifier\", KNeighborsClassifier(),\n",
    "                                                    tuned_parameters, transformed_train_data, train_labels,\n",
    "                                                    useRandomizedSearch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "TESTING DATA RESULTS:\n",
      "\n",
      "\n",
      "Acc\tF1\tPrec\tRecall\tModel\n",
      "0.5417\t0.4500\t0.6429\t0.3462\tBaggingClassifier()\n",
      "0.4167\t0.3000\t0.4286\t0.2308\tMultinomialNB()\n",
      "0.4375\t0.2703\t0.4545\t0.1923\tLogisticRegression()\n",
      "0.5000\t0.2500\t0.6667\t0.1538\tAdaBoostClassifier()\n",
      "0.4375\t0.4490\t0.4783\t0.4231\tRandomForestClassifier()\n",
      "0.5000\t0.2500\t0.6667\t0.1538\tGradientBoostingClassifier()\n",
      "0.5625\t0.5532\t0.6190\t0.5000\tDecisionTreeClassifier()\n",
      "0.4583\t0.3810\t0.5000\t0.3077\tMLPClassifier()\n",
      "0.4583\t0.4091\t0.5000\t0.3462\tSVC()\n",
      "0.5208\t0.6462\t0.5385\t0.8077\tKNeighborsClassifier()\n"
     ]
    }
   ],
   "source": [
    "def run_model(name, model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "#     print()\n",
    "#     try:\n",
    "#         print(\"Name: \" + name + \"\\nFeature Importance:\")\n",
    "#         print(model.feature_importances_)\n",
    "#     except:\n",
    "#         print(\"Feature Importance missing.\")\n",
    "    return accuracy_score(y_test, y_predict), f1_score(y_test, y_predict), precision_score(y_test,\n",
    "                                                                                           y_predict), recall_score(\n",
    "        y_test, y_predict)\n",
    "\n",
    "\n",
    "def run_test(models_dict, X_train, X_test, y_train, y_test):\n",
    "    print(\"Acc\\tF1\\tPrec\\tRecall\\tModel\")\n",
    "    for name in models_dict.keys():\n",
    "        model = eval(name)\n",
    "        model.set_params(**models_dict[name])\n",
    "        acc, f1, prec, rec = run_model(name, model, X_train, X_test, y_train, y_test)\n",
    "        print(\"%.4f\\t%.4f\\t%.4f\\t%.4f\\t%s\" % (acc, f1, prec, rec, name))\n",
    "        \n",
    "print(\"\\n\\n\\nTESTING DATA RESULTS:\\n\\n\")\n",
    "run_test(models_dict, transformed_train_data, transformed_test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
